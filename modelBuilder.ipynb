{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = [\"braunglas\", \"gruenglas\", \"weissglas\", \"keinglas\"]\n",
    "\n",
    "# Load the MNIST dataset\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each CSV file in the directory\n",
    "for filename in os.listdir(\".\"):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Extract the label from the filename (e.g., label1.csv -> label1)\n",
    "        label = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(filename, index_col=None)\n",
    "        \n",
    "        # Add a column for the label\n",
    "        df['label'] = label\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dataframes.append(df)\n",
    "\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "combined_df = combined_df.sample(frac=1).reset_index(drop=False)\n",
    "\n",
    "# Example: Separate features and labels\n",
    "X = combined_df.drop(columns=['label', \"index\"]).values\n",
    "\n",
    "# y is 4 one-hot encoded columns for the 3 classes and 1 for the no class\n",
    "y = np.array([all_labels.index(v) for v in combined_df['label'].values])\n",
    "\n",
    "y = to_categorical(y, num_classes=4)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(x_train.shape[1], activation='relu', input_shape=(x_train.shape[1],)))\n",
    "# model.add(Dense(1, activation='relu', input_shape=(1,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"nadam\",\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09803922 0.41176471 0.16862745 0.25139505 0.77706186 0.16586426] [0. 1. 0. 0.]\n",
      "[0.67058824 0.42745098 0.30980392 0.3950544  0.56969085 0.04040039] [0. 0. 1. 0.]\n",
      "[0.08235294 0.1254902  0.09803922 0.60830447 0.82142652 0.06989746] [0. 0. 0. 1.]\n",
      "[0.29019608 0.23137255 0.18823529 0.27455813 0.79593257 0.1768929 ] [1. 0. 0. 0.]\n",
      "[0.91764706 0.90196078 0.82745098 0.60996545 0.77907827 0.04506836] [0. 0. 1. 0.]\n",
      "[0.76078431 0.43529412 0.31764706 0.88281592 0.79919461 0.03359375] [0. 0. 1. 0.]\n",
      "[0.29019608 0.36862745 0.29803922 0.32485402 0.787373   0.19778646] [1. 0. 0. 0.]\n",
      "[0.04705882 0.24313725 0.09803922 0.2572529  0.71464819 0.21820964] [0. 1. 0. 0.]\n",
      "[0.03137255 0.26666667 0.09803922 0.26503609 0.76568914 0.21554199] [0. 1. 0. 0.]\n",
      "[0.23529412 0.21568627 0.22352941 0.67283071 0.90538392 0.10751139] [0. 0. 0. 1.]\n",
      "[0.17254902 0.18039216 0.19215686 0.43250303 0.8586694  0.10763835] [0. 0. 0. 1.]\n",
      "[0.37254902 0.45098039 0.30588235 0.27104617 0.69949024 0.24946777] [0. 1. 0. 0.]\n",
      "[0.12156863 0.41960784 0.2        0.27108543 0.70389987 0.1732666 ] [0. 1. 0. 0.]\n",
      "[0.25490196 0.28235294 0.2627451  0.30898844 0.81013795 0.18505534] [1. 0. 0. 0.]\n",
      "[0.29019608 0.36470588 0.27843137 0.40175482 0.58256092 0.07938151] [0. 0. 0. 1.]\n",
      "[0.26666667 0.29803922 0.30196078 0.66735622 0.80485953 0.22606283] [0. 0. 0. 1.]\n",
      "[0.1372549  0.17254902 0.19215686 0.43217989 0.77096203 0.02620443] [0. 0. 0. 1.]\n",
      "[0.34901961 0.41176471 0.40392157 0.84907253 0.8623724  0.07672363] [0. 0. 0. 1.]\n",
      "[0.74117647 0.7254902  0.69411765 0.46273313 0.77976771 0.05725911] [0. 0. 1. 0.]\n",
      "[0.19607843 0.25098039 0.21176471 0.31001298 0.78926066 0.18566895] [1. 0. 0. 0.]\n",
      "[0.31764706 0.36862745 0.29411765 0.30255627 0.82248682 0.17673828] [1. 0. 0. 0.]\n",
      "[0.10980392 0.42352941 0.17254902 0.23922048 0.76952582 0.18906738] [0. 1. 0. 0.]\n",
      "[0.04705882 0.22745098 0.4627451  0.4945802  0.67361852 0.12948079] [0. 0. 0. 1.]\n",
      "[0.57647059 0.75686275 0.56078431 0.74283268 0.78361315 0.09191895] [0. 0. 0. 1.]\n",
      "[0.10588235 0.25882353 0.0745098  0.2450556  0.78650807 0.20642253] [0. 1. 0. 0.]\n",
      "[0.74509804 0.83529412 0.69411765 0.5693344  0.81796499 0.06419108] [0. 0. 1. 0.]\n",
      "[0.24313725 0.2        0.17647059 0.2542     0.75982178 0.16903809] [1. 0. 0. 0.]\n",
      "[0.43529412 0.49411765 0.48235294 0.31926193 0.75445924 0.19758138] [1. 0. 0. 0.]\n",
      "[0.79215686 0.68627451 0.6745098  0.41897497 0.66975717 0.05279134] [0. 0. 1. 0.]\n",
      "[0.76078431 0.79215686 0.74509804 0.60025522 0.83829876 0.0651237 ] [0. 0. 1. 0.]\n",
      "[0.51764706 0.57254902 0.43921569 0.4347002  0.9029304  0.00160482] [0. 0. 0. 1.]\n",
      "[0.80784314 0.82352941 0.78431373 0.64127653 0.79524854 0.05312012] [0. 0. 1. 0.]\n",
      "[0.29803922 0.25882353 0.23921569 0.25301608 0.7707971  0.18033691] [1. 0. 0. 0.]\n",
      "[0.78823529 0.70196078 0.69411765 0.41622113 0.69805298 0.05339355] [0. 0. 1. 0.]\n",
      "[0.90196078 0.89803922 0.83921569 0.59732429 0.78276326 0.04730469] [0. 0. 1. 0.]\n",
      "[0.38823529 0.44313725 0.44313725 0.65892118 0.79109756 0.25886556] [0. 0. 0. 1.]\n",
      "[0.14901961 0.40784314 0.21176471 0.26131801 0.80145396 0.16436198] [0. 1. 0. 0.]\n",
      "[0.17254902 0.24313725 0.22352941 0.3323605  0.74808294 0.2007015 ] [1. 0. 0. 0.]\n",
      "[0.08235294 0.39607843 0.14901961 0.255211   0.73015873 0.16052083] [0. 1. 0. 0.]\n",
      "[0.23137255 0.26666667 0.25098039 0.30826634 0.82370573 0.19233073] [1. 0. 0. 0.]\n",
      "[0.1372549  0.17647059 0.16078431 0.32431124 0.74807843 0.18628906] [1. 0. 0. 0.]\n",
      "[0.83529412 0.69019608 0.72941176 0.38818452 0.68507362 0.04460449] [0. 0. 1. 0.]\n",
      "[0.84705882 0.85490196 0.79215686 0.67141421 0.80532607 0.04616862] [0. 0. 1. 0.]\n",
      "[0.78823529 0.8        0.70588235 0.55855058 0.85184871 0.06539714] [0. 0. 1. 0.]\n",
      "[0.31764706 0.35686275 0.34509804 0.3204779  0.75817733 0.19512207] [1. 0. 0. 0.]\n",
      "[0.33333333 0.39215686 0.30196078 0.3236697  0.77503863 0.18124349] [1. 0. 0. 0.]\n",
      "[0.30588235 0.39607843 0.19607843 0.24171533 0.78577072 0.21366211] [0. 1. 0. 0.]\n",
      "[0.27058824 0.29411765 0.27058824 0.32704719 0.79785854 0.18313477] [1. 0. 0. 0.]\n",
      "[0.39607843 0.45490196 0.29803922 0.2594029  0.71889065 0.23085938] [0. 1. 0. 0.]\n",
      "[0.0627451  0.34509804 0.10980392 0.2503353  0.81494751 0.20214518] [0. 1. 0. 0.]\n",
      "[0.62352941 0.71372549 0.67058824 0.683055   0.75589084 0.02772461] [0. 0. 1. 0.]\n",
      "[0.42352941 0.44313725 0.41960784 0.95492966 0.82117081 0.04      ] [0. 0. 0. 1.]\n",
      "[0.3372549  0.29019608 0.25490196 0.26540109 0.72466674 0.1723584 ] [1. 0. 0. 0.]\n",
      "[0.89019608 0.91372549 0.88235294 0.84255046 0.9050338  0.12615234] [0. 0. 0. 1.]\n",
      "[0.78823529 0.79607843 0.71764706 0.50726832 0.80035633 0.06507324] [0. 0. 1. 0.]\n",
      "[0.09019608 0.38431373 0.17647059 0.26757946 0.76535234 0.20844889] [0. 1. 0. 0.]\n",
      "[0.27058824 0.30196078 0.28627451 0.30847522 0.81511437 0.18322428] [1. 0. 0. 0.]\n",
      "[0.6627451  0.7254902  0.71372549 0.67492928 0.73025174 0.03286133] [0. 0. 1. 0.]\n",
      "[0.65882353 0.79215686 0.57647059 0.82031939 0.80554335 0.05368978] [0. 0. 0. 1.]\n",
      "[0.28235294 0.32941176 0.3372549  0.87122123 0.88558118 0.06899577] [0. 0. 0. 1.]\n",
      "[0.30196078 0.26666667 0.23921569 0.26142497 0.71649562 0.1711263 ] [1. 0. 0. 0.]\n",
      "[0.6        0.67058824 0.63921569 0.45862132 0.53452015 0.04412923] [0. 0. 0. 1.]\n",
      "[0.06666667 0.28235294 0.21960784 0.57868737 0.72378374 0.01479492] [0. 0. 0. 1.]\n",
      "[0.8        0.42352941 0.29411765 0.84718145 0.81579735 0.03548665] [0. 0. 1. 0.]\n",
      "[0.61176471 0.35294118 0.25098039 0.89631528 0.79185685 0.03190755] [0. 0. 1. 0.]\n",
      "[0.78823529 0.41960784 0.31764706 0.88197674 0.82396627 0.03467122] [0. 0. 1. 0.]\n",
      "[0.30980392 0.2627451  0.23921569 0.24941262 0.75845291 0.17911784] [1. 0. 0. 0.]\n",
      "[0.16470588 0.43921569 0.20392157 0.25399459 0.77889544 0.15211914] [0. 1. 0. 0.]\n",
      "[0.8745098  0.8745098  0.80784314 0.59506356 0.79439201 0.04712565] [0. 0. 1. 0.]\n",
      "[0.40784314 0.35686275 0.29019608 0.29841802 0.66737849 0.19380046] [1. 0. 0. 0.]\n",
      "[0.16078431 0.20784314 0.19607843 0.33456372 0.74593679 0.1905599 ] [1. 0. 0. 0.]\n",
      "[0.78431373 0.39215686 0.23529412 0.82326662 0.81759868 0.03236328] [0. 0. 1. 0.]\n",
      "[0.76862745 0.79215686 0.74901961 0.68633314 0.82993754 0.04952474] [0. 0. 1. 0.]\n",
      "[0.76862745 0.32156863 0.19607843 0.93927507 0.79447198 0.03574219] [0. 0. 1. 0.]\n",
      "[0.55294118 0.56470588 0.50196078 0.62677431 0.68431906 0.05077148] [0. 0. 0. 1.]\n",
      "[0.38431373 0.36470588 0.28627451 0.31712742 0.7858727  0.22262044] [1. 0. 0. 0.]\n",
      "[0.87843137 0.87843137 0.81568627 0.62295085 0.79039551 0.04495117] [0. 0. 1. 0.]\n",
      "[0.77254902 0.38039216 0.27058824 0.92521098 0.82126623 0.0329362 ] [0. 0. 1. 0.]\n",
      "[0.75686275 0.36470588 0.28627451 0.89928317 0.81898567 0.03422038] [0. 0. 1. 0.]\n",
      "[0.7372549  0.79607843 0.70980392 0.47875385 0.76651574 0.05924154] [0. 0. 1. 0.]\n",
      "[0.82352941 0.72941176 0.69019608 0.44143847 0.74702482 0.03575846] [0. 0. 1. 0.]\n",
      "[0.32941176 0.3372549  0.2627451  0.33263856 0.77320462 0.15578613] [0. 0. 0. 1.]\n",
      "[0.31764706 0.38823529 0.30588235 0.3209875  0.80462508 0.18750488] [1. 0. 0. 0.]\n",
      "[0.65098039 0.71764706 0.69803922 0.54212646 0.84988453 0.01677083] [0. 0. 1. 0.]\n",
      "[0.81960784 0.71372549 0.71764706 0.44267837 0.70323208 0.04709961] [0. 0. 1. 0.]\n",
      "[0.13333333 0.40392157 0.16862745 0.25013068 0.78131572 0.15479818] [0. 1. 0. 0.]\n",
      "[0.09411765 0.38823529 0.15294118 0.24270902 0.74645952 0.19182454] [0. 1. 0. 0.]\n",
      "[0.78431373 0.8        0.71372549 0.56407914 0.78358621 0.06001628] [0. 0. 1. 0.]\n",
      "[0.14901961 0.21568627 0.21568627 0.72668968 0.8468584  0.01504883] [0. 0. 0. 1.]\n",
      "[0.08627451 0.12941176 0.12941176 0.33119076 0.76480997 0.20330078] [1. 0. 0. 0.]\n",
      "[0.05882353 0.2745098  0.17254902 0.27384114 0.74329687 0.23228027] [0. 1. 0. 0.]\n",
      "[0.88235294 0.88235294 0.82352941 0.62084182 0.79996456 0.05142741] [0. 0. 1. 0.]\n",
      "[0.04313725 0.0745098  0.07843137 0.55599548 0.91791841 0.11627116] [0. 0. 0. 1.]\n",
      "[0.51764706 0.38431373 0.19607843 0.84059275 0.78148674 0.04835449] [0. 0. 0. 1.]\n",
      "[0.69019608 0.36862745 0.23921569 0.90828842 0.79098329 0.03343913] [0. 0. 1. 0.]\n",
      "[0.10196078 0.42352941 0.16078431 0.25218078 0.74826855 0.15861491] [0. 1. 0. 0.]\n",
      "[0.25882353 0.37254902 0.19607843 0.24537979 0.7577035  0.22584473] [0. 1. 0. 0.]\n",
      "[0.89411765 0.89019608 0.81960784 0.51435703 0.74588277 0.04747233] [0. 0. 1. 0.]\n",
      "[0.39215686 0.43529412 0.24313725 0.25358935 0.71196279 0.23495931] [0. 1. 0. 0.]\n",
      "[0.06666667 0.10980392 0.12156863 0.31756937 0.80020609 0.1870638 ] [1. 0. 0. 0.]\n",
      "[0.12156863 0.4627451  0.19607843 0.25341488 0.70612772 0.19314616] [0. 1. 0. 0.]\n",
      "[0.0745098  0.38039216 0.1254902  0.26512488 0.75409184 0.15747559] [0. 1. 0. 0.]\n",
      "[0.72941176 0.76470588 0.73333333 0.73354712 0.81249508 0.05044108] [0. 0. 1. 0.]\n",
      "[0.65490196 0.69411765 0.57647059 0.47450946 0.43962306 0.05148112] [0. 0. 0. 1.]\n",
      "[0.81960784 0.42745098 0.30196078 0.88520621 0.80679245 0.03479818] [0. 0. 1. 0.]\n",
      "[0.16862745 0.21176471 0.18431373 0.30840415 0.7912366  0.18166504] [1. 0. 0. 0.]\n",
      "[0.12156863 0.12941176 0.10980392 0.30707283 0.82485592 0.17937663] [1. 0. 0. 0.]\n",
      "[0.42745098 0.50980392 0.41960784 0.73989783 0.75399872 0.01150879] [0. 0. 0. 1.]\n",
      "[0.62352941 0.44313725 0.33333333 0.38743432 0.79505409 0.08445638] [0. 0. 0. 1.]\n",
      "[0.80392157 0.71764706 0.74509804 0.51310768 0.75365079 0.04250326] [0. 0. 1. 0.]\n",
      "[0.38431373 0.45882353 0.47843137 0.30573874 0.67255929 0.20651204] [1. 0. 0. 0.]\n",
      "[0.81568627 0.82745098 0.76078431 0.56839977 0.75759398 0.04919922] [0. 0. 1. 0.]\n",
      "[0.78039216 0.44313725 0.28235294 0.78692895 0.80582969 0.03721191] [0. 0. 1. 0.]\n",
      "[0.4        0.36862745 0.28235294 0.31604645 0.7570529  0.2168099 ] [1. 0. 0. 0.]\n",
      "[0.12941176 0.15686275 0.15294118 0.32133607 0.80923362 0.18458008] [1. 0. 0. 0.]\n",
      "[0.13333333 0.52941176 0.25098039 0.27232586 0.7769606  0.23745605] [0. 1. 0. 0.]\n",
      "[0.09411765 0.25882353 0.09411765 0.24987983 0.83209846 0.21788411] [0. 1. 0. 0.]\n",
      "[0.29019608 0.23529412 0.18431373 0.27632935 0.80692329 0.17380371] [1. 0. 0. 0.]\n",
      "[0.10588235 0.25882353 0.12941176 0.24980787 0.74117004 0.21189453] [0. 1. 0. 0.]\n",
      "[0.02745098 0.23529412 0.08627451 0.25619599 0.73854791 0.21882487] [0. 1. 0. 0.]\n",
      "[0.41960784 0.45490196 0.2745098  0.26152252 0.71580386 0.23274577] [0. 1. 0. 0.]\n",
      "[0.04705882 0.2627451  0.08627451 0.25009203 0.81030404 0.19624023] [0. 1. 0. 0.]\n",
      "[0.16470588 0.32156863 0.15686275 0.2435976  0.71011293 0.22720378] [0. 1. 0. 0.]\n",
      "[0.16470588 0.20784314 0.20784314 0.74729582 0.84312697 0.01046224] [0. 0. 0. 1.]\n",
      "[0.61960784 0.65882353 0.65490196 0.77239295 0.79737202 0.08543457] [0. 0. 0. 1.]\n",
      "[0.29803922 0.36078431 0.28235294 0.31640686 0.81333754 0.1848291 ] [1. 0. 0. 0.]\n",
      "[0.08627451 0.16078431 0.17254902 0.34650815 0.79147941 0.20924479] [1. 0. 0. 0.]\n",
      "[0.7254902  0.72941176 0.75294118 0.78268646 0.80890284 0.02978353] [0. 0. 0. 1.]\n",
      "[0.07058824 0.32156863 0.10196078 0.2509404  0.822964   0.19690592] [0. 1. 0. 0.]\n",
      "[0.70980392 0.40784314 0.29411765 0.85841817 0.80660598 0.03374512] [0. 0. 1. 0.]\n",
      "[0.63529412 0.71764706 0.71764706 0.80545586 0.7767082  0.03480143] [0. 0. 1. 0.]\n",
      "[0.14901961 0.42745098 0.2        0.25254462 0.79174787 0.16662272] [0. 1. 0. 0.]\n",
      "[0.11372549 0.2        0.18823529 0.65732237 0.67774693 0.28526042] [0. 0. 0. 1.]\n",
      "[0.12156863 0.10196078 0.11764706 0.94150768 0.80023909 0.07626628] [0. 0. 0. 1.]\n",
      "[0.85882353 0.70196078 0.41568627 0.12571521 0.83821359 0.11647949] [0. 0. 0. 1.]\n",
      "[0.10196078 0.14901961 0.16470588 0.91469795 0.82483777 0.13695475] [0. 0. 0. 1.]\n",
      "[0.24313725 0.30980392 0.2745098  0.31514264 0.80131169 0.17798014] [1. 0. 0. 0.]\n",
      "[0.10980392 0.1254902  0.11372549 0.31880721 0.80884659 0.18467611] [1. 0. 0. 0.]\n",
      "[0.20784314 0.25098039 0.25882353 0.4510698  0.73224902 0.02806478] [0. 0. 0. 1.]\n",
      "[0.27058824 0.33333333 0.34901961 0.35490676 0.76092169 0.20558757] [1. 0. 0. 0.]\n",
      "[0.2745098  0.27058824 0.21568627 0.29789377 0.76441879 0.19654297] [1. 0. 0. 0.]\n",
      "[0.85490196 0.85882353 0.80392157 0.59730261 0.79126972 0.04947754] [0. 0. 1. 0.]\n",
      "[0.2        0.25490196 0.21176471 0.32402454 0.80727073 0.18927897] [1. 0. 0. 0.]\n",
      "[0.19607843 0.44705882 0.24705882 0.27265651 0.720848   0.15661784] [0. 1. 0. 0.]\n",
      "[0.36078431 0.38823529 0.32941176 0.86623806 0.87357408 0.07827474] [0. 0. 0. 1.]\n",
      "[0.26666667 0.29411765 0.2745098  0.32672833 0.80047959 0.17984212] [1. 0. 0. 0.]\n",
      "[0.77647059 0.44313725 0.36470588 0.83995222 0.82306881 0.03301921] [0. 0. 1. 0.]\n",
      "[0.46666667 0.14117647 0.16862745 0.13733914 0.41481447 0.11488281] [0. 0. 0. 1.]\n",
      "[0.81568627 0.69803922 0.70588235 0.48587648 0.7683434  0.04390462] [0. 0. 1. 0.]\n",
      "[0.78431373 0.69019608 0.71764706 0.54141913 0.75596372 0.02713053] [0. 0. 1. 0.]\n",
      "[0.12941176 0.49019608 0.21568627 0.24603792 0.69192656 0.18615234] [0. 1. 0. 0.]\n",
      "[0.3372549  0.16470588 0.15686275 0.22417218 0.44839607 0.18883138] [0. 0. 0. 1.]\n",
      "[0.64705882 0.34509804 0.17647059 0.71704308 0.82041222 0.02552572] [0. 0. 1. 0.]\n",
      "[0.01960784 0.25490196 0.11372549 0.26772849 0.73206544 0.22867513] [0. 1. 0. 0.]\n",
      "[0.18431373 0.44705882 0.24313725 0.25700904 0.78840023 0.16823568] [0. 1. 0. 0.]\n",
      "[0.81176471 0.6745098  0.70980392 0.49643459 0.76505115 0.06597819] [0. 0. 1. 0.]\n",
      "[0.7372549  0.4        0.26666667 0.86041512 0.8139676  0.0349235 ] [0. 0. 1. 0.]\n",
      "[0.8745098  0.88235294 0.83137255 0.62487037 0.82239656 0.04726237] [0. 0. 1. 0.]\n",
      "[0.09411765 0.39607843 0.16470588 0.26582891 0.82524337 0.19965007] [0. 1. 0. 0.]\n",
      "[0.18039216 0.20784314 0.18823529 0.95645625 0.83235344 0.04792806] [0. 0. 0. 1.]\n",
      "[0.41568627 0.36470588 0.2627451  0.27055171 0.78861789 0.19576823] [1. 0. 0. 0.]\n",
      "[0.06666667 0.38039216 0.11372549 0.2502836  0.7917306  0.16125326] [0. 1. 0. 0.]\n",
      "[0.77647059 0.40392157 0.25490196 0.89291361 0.80070283 0.02966797] [0. 0. 1. 0.]\n",
      "[0.17254902 0.44705882 0.21176471 0.25547026 0.78350558 0.15300293] [0. 1. 0. 0.]\n",
      "[0.08235294 0.37254902 0.14901961 0.25057926 0.81235786 0.20523275] [0. 1. 0. 0.]\n",
      "[0.84705882 0.77254902 0.37254902 0.56470992 0.89835165 0.01330566] [0. 0. 0. 1.]\n",
      "[0.37254902 0.43921569 0.25098039 0.26458979 0.70936115 0.24352539] [0. 1. 0. 0.]\n",
      "[0.32156863 0.29019608 0.25490196 0.2951777  0.78983326 0.19475098] [1. 0. 0. 0.]\n",
      "[0.67843137 0.74117647 0.7372549  0.59789207 0.68455153 0.02201172] [0. 0. 1. 0.]\n",
      "[0.27843137 0.34901961 0.36470588 0.62414428 0.80316318 0.04397135] [0. 0. 0. 1.]\n",
      "[0.76470588 0.39215686 0.25882353 0.94330251 0.79830141 0.03472819] [0. 0. 1. 0.]\n",
      "[0.09411765 0.1372549  0.10588235 0.95444846 0.82362712 0.03874023] [0. 0. 0. 1.]\n",
      "[0.4        0.46666667 0.45882353 0.32201165 0.75368026 0.20090658] [1. 0. 0. 0.]\n",
      "[0.21960784 0.25490196 0.25490196 0.32229662 0.81549326 0.18513184] [1. 0. 0. 0.]\n",
      "[0.3254902  0.2745098  0.25098039 0.26059201 0.71945878 0.17058105] [1. 0. 0. 0.]\n",
      "[0.02352941 0.24313725 0.10588235 0.26416437 0.74531725 0.21028646] [0. 1. 0. 0.]\n",
      "[0.31372549 0.27058824 0.24313725 0.25539622 0.74892856 0.17520345] [1. 0. 0. 0.]\n",
      "[0.24313725 0.36862745 0.19215686 0.26459393 0.72485713 0.24029297] [0. 1. 0. 0.]\n",
      "[0.29411765 0.2745098  0.2627451  0.28190085 0.75381311 0.18452962] [1. 0. 0. 0.]\n",
      "[0.11764706 0.45882353 0.28627451 0.26596437 0.73852512 0.21783366] [0. 1. 0. 0.]\n",
      "[0.17254902 0.43921569 0.25098039 0.27585453 0.71378174 0.17914714] [0. 1. 0. 0.]\n",
      "[0.05098039 0.26666667 0.10588235 0.25532193 0.74090625 0.20617188] [0. 1. 0. 0.]\n",
      "[0.05490196 0.28627451 0.08627451 0.24685656 0.82772407 0.19510254] [0. 1. 0. 0.]\n",
      "[0.31372549 0.2627451  0.2627451  0.28409469 0.76477051 0.18013184] [1. 0. 0. 0.]\n",
      "[0.11372549 0.39215686 0.16470588 0.25290853 0.78513938 0.16035807] [0. 1. 0. 0.]\n",
      "[0.65882353 0.72941176 0.70980392 0.62048323 0.71797626 0.02284342] [0. 0. 1. 0.]\n",
      "[0.03529412 0.27058824 0.10196078 0.26216372 0.74466816 0.20720052] [0. 1. 0. 0.]\n",
      "[0.8        0.81568627 0.75686275 0.56184533 0.85783333 0.01675456] [0. 0. 1. 0.]\n",
      "[0.31372549 0.28235294 0.26666667 0.55141499 0.73718643 0.14617025] [0. 0. 0. 1.]\n",
      "[0.12941176 0.16470588 0.15294118 0.34788241 0.76825182 0.20151855] [1. 0. 0. 0.]\n",
      "[0.28627451 0.36862745 0.21960784 0.25714497 0.67676722 0.21963542] [0. 1. 0. 0.]\n",
      "[0.29411765 0.28235294 0.2627451  0.27933648 0.75705029 0.19014974] [1. 0. 0. 0.]\n",
      "[0.70980392 0.38431373 0.27058824 0.84189818 0.79009947 0.0330957 ] [0. 0. 1. 0.]\n",
      "[0.25490196 0.29019608 0.27843137 0.33041177 0.76930537 0.18979329] [1. 0. 0. 0.]\n",
      "[0.83529412 0.65098039 0.4        0.8263982  0.90862463 0.14503092] [0. 0. 0. 1.]\n",
      "[0.78039216 0.79215686 0.74509804 0.52107941 0.83377284 0.01440918] [0. 0. 1. 0.]\n",
      "[0.01960784 0.24705882 0.07843137 0.2429469  0.74058242 0.20894694] [0. 1. 0. 0.]\n",
      "[0.08627451 0.39607843 0.11764706 0.25118876 0.760552   0.15168457] [0. 1. 0. 0.]\n",
      "[0.0745098  0.20392157 0.27058824 0.62602455 0.79718615 0.0179834 ] [0. 0. 0. 1.]\n",
      "[0.25490196 0.28627451 0.27058824 0.31393995 0.81824621 0.18802083] [1. 0. 0. 0.]\n",
      "[0.14117647 0.18039216 0.17254902 0.92808992 0.80597015 0.00641602] [0. 0. 0. 1.]\n",
      "[0.16862745 0.52941176 0.25098039 0.24091594 0.78101462 0.18361979] [0. 1. 0. 0.]\n",
      "[0.1372549  0.18039216 0.12156863 0.93459921 0.81607143 0.03793457] [0. 0. 0. 1.]\n",
      "[0.12156863 0.41960784 0.18039216 0.26241469 0.80450177 0.15846354] [0. 1. 0. 0.]\n",
      "[0.10196078 0.41568627 0.16470588 0.26022397 0.72695826 0.15973796] [0. 1. 0. 0.]\n",
      "[0.83137255 0.74509804 0.75686275 0.65582744 0.75507849 0.02661947] [0. 0. 1. 0.]\n",
      "[0.64705882 0.7254902  0.69803922 0.68737252 0.77569347 0.02530599] [0. 0. 1. 0.]\n",
      "[0.38039216 0.43921569 0.29019608 0.26791957 0.66322748 0.24168294] [0. 1. 0. 0.]\n",
      "[0.24313725 0.29019608 0.22745098 0.29845434 0.81445704 0.18923665] [1. 0. 0. 0.]\n",
      "[0.71764706 0.75686275 0.7254902  0.75465787 0.8192016  0.05063477] [0. 0. 1. 0.]\n",
      "[0.17254902 0.42745098 0.22352941 0.26247311 0.76067835 0.16243652] [0. 1. 0. 0.]\n",
      "[0.19607843 0.24705882 0.21176471 0.30500587 0.80397005 0.18418294] [1. 0. 0. 0.]\n",
      "[0.28627451 0.2627451  0.23137255 0.26603168 0.74145247 0.17690592] [1. 0. 0. 0.]\n",
      "[0.1254902  0.17254902 0.15686275 0.56616327 0.91761205 0.15007975] [0. 0. 0. 1.]\n",
      "[0.84705882 0.77254902 0.37254902 0.5595291  0.9        0.01318359] [0. 0. 0. 1.]\n",
      "[0.35294118 0.43921569 0.45490196 0.36165856 0.76563801 0.2094987 ] [1. 0. 0. 0.]\n",
      "[0.11764706 0.14901961 0.11764706 0.57373492 0.91579835 0.15020345] [0. 0. 0. 1.]\n",
      "[0.11372549 0.14509804 0.1372549  0.91160415 0.82718228 0.0083903 ] [0. 0. 0. 1.]\n",
      "[0.49803922 0.30980392 0.29019608 0.28445415 0.75903403 0.04916178] [0. 0. 0. 1.]\n",
      "[0.28235294 0.26666667 0.24313725 0.27732223 0.73853603 0.18297038] [1. 0. 0. 0.]\n",
      "[0.09411765 0.10980392 0.1254902  0.32473144 0.81538427 0.1772054 ] [1. 0. 0. 0.]\n",
      "[0.89019608 0.90980392 0.73333333 0.63352875 0.86071946 0.07839355] [0. 0. 0. 1.]\n",
      "[0.01960784 0.24313725 0.09019608 0.24737144 0.76705279 0.21275228] [0. 1. 0. 0.]\n",
      "[0.79607843 0.81568627 0.76078431 0.68090427 0.82730335 0.05515625] [0. 0. 1. 0.]\n",
      "[0.89411765 0.9254902  0.84313725 0.93575247 0.79405095 0.07751302] [0. 0. 0. 1.]\n",
      "[0.         0.02352941 0.04705882 0.38310343 0.88818492 0.074611  ] [0. 0. 0. 1.]\n",
      "[0.05098039 0.38431373 0.10588235 0.25225661 0.7877507  0.16774577] [0. 1. 0. 0.]\n",
      "[0.03137255 0.25882353 0.08627451 0.27315036 0.75335246 0.21588379] [0. 1. 0. 0.]\n",
      "[0.0627451  0.26666667 0.1254902  0.26113547 0.65238695 0.21236979] [0. 1. 0. 0.]\n",
      "[0.75686275 0.79215686 0.69803922 0.56934147 0.79307038 0.06057617] [0. 0. 1. 0.]\n",
      "[0.30588235 0.25882353 0.25098039 0.28634382 0.77435269 0.18010254] [1. 0. 0. 0.]\n",
      "[0.30196078 0.42352941 0.26666667 0.28843198 0.64245197 0.24296061] [0. 1. 0. 0.]\n",
      "[0.06666667 0.09803922 0.10588235 0.8081823  0.83415468 0.04049805] [0. 0. 0. 1.]\n",
      "[0.07058824 0.38039216 0.14117647 0.26130562 0.81334262 0.17104818] [0. 1. 0. 0.]\n",
      "[0.31372549 0.25098039 0.43529412 0.34948982 0.77507199 0.06133464] [0. 0. 0. 1.]\n",
      "[0.25882353 0.28235294 0.27058824 0.30179249 0.82360543 0.18981934] [1. 0. 0. 0.]\n",
      "[0.04705882 0.30980392 0.10588235 0.25253342 0.78390682 0.19815592] [0. 1. 0. 0.]\n",
      "[0.67058824 0.74117647 0.7254902  0.79170574 0.80356353 0.04433594] [0. 0. 1. 0.]\n",
      "[0.2745098  0.30588235 0.29019608 0.32144178 0.80572448 0.18308594] [1. 0. 0. 0.]\n",
      "[0.31764706 0.38823529 0.30980392 0.3046549  0.8026199  0.18548828] [1. 0. 0. 0.]\n",
      "[0.80392157 0.80784314 0.7372549  0.51059482 0.81903344 0.01522624] [0. 0. 1. 0.]\n",
      "[0.04313725 0.26666667 0.1254902  0.26008318 0.7803914  0.21755697] [0. 1. 0. 0.]\n",
      "[0.40784314 0.41568627 0.22745098 0.8165846  0.81817123 0.01143229] [0. 0. 0. 1.]\n",
      "[0.18431373 0.22352941 0.19215686 0.32668646 0.80463913 0.17367188] [1. 0. 0. 0.]\n",
      "[0.40784314 0.25098039 0.22352941 0.25082136 0.77470639 0.04401855] [0. 0. 0. 1.]\n",
      "[0.82745098 0.68627451 0.69411765 0.49542379 0.78991228 0.04103841] [0. 0. 1. 0.]\n",
      "[0.36862745 0.45490196 0.29019608 0.2672446  0.66326984 0.2427002 ] [0. 1. 0. 0.]\n",
      "[0.14901961 0.18823529 0.13333333 0.87142184 0.86140664 0.0737972 ] [0. 0. 0. 1.]\n",
      "[0.27058824 0.32941176 0.27058824 0.68981838 0.88750236 0.2062793 ] [0. 0. 0. 1.]\n",
      "[0.60392157 0.63137255 0.54901961 0.38434907 0.86693062 0.11226074] [0. 0. 0. 1.]\n",
      "[0.14901961 0.2        0.19215686 0.92702087 0.82189989 0.14110352] [0. 0. 0. 1.]\n",
      "[0.30588235 0.25098039 0.25098039 0.2832827  0.78475314 0.17674316] [1. 0. 0. 0.]\n",
      "[0.4        0.51372549 0.42745098 0.64293783 0.79835622 0.01391276] [0. 0. 0. 1.]\n",
      "[0.86666667 0.85882353 0.78431373 0.63049147 0.80134328 0.05107259] [0. 0. 1. 0.]\n",
      "[0.21568627 0.25882353 0.28235294 0.51922846 0.81256758 0.16635417] [0. 0. 0. 1.]\n",
      "[0.11372549 0.42352941 0.17647059 0.24755022 0.74724906 0.15949219] [0. 1. 0. 0.]\n",
      "[0.64705882 0.71764706 0.69019608 0.58166465 0.7407726  0.03083659] [0. 0. 1. 0.]\n",
      "[0.14901961 0.51372549 0.24313725 0.22427325 0.72995096 0.17344889] [0. 1. 0. 0.]\n",
      "[0.74901961 0.82745098 0.70196078 0.5874582  0.80611874 0.06128418] [0. 0. 1. 0.]\n",
      "[0.76078431 0.7372549  0.7254902  0.41878783 0.70120802 0.0490332 ] [0. 0. 1. 0.]\n",
      "[0.44313725 0.34901961 0.28235294 0.27004079 0.70529434 0.18098307] [1. 0. 0. 0.]\n",
      "[0.3372549  0.38039216 0.4        0.88205313 0.8759478  0.0714502 ] [0. 0. 0. 1.]\n",
      "[0.81176471 0.84313725 0.81568627 0.63668908 0.80412582 0.04485514] [0. 0. 0. 1.]\n",
      "[0.70588235 0.72941176 0.66666667 0.49935136 0.81295539 0.01489095] [0. 0. 1. 0.]\n",
      "[0.81960784 0.72156863 0.70588235 0.47070566 0.79845349 0.03899089] [0. 0. 1. 0.]\n",
      "[0.11372549 0.42352941 0.19215686 0.26345409 0.80874331 0.19932943] [0. 1. 0. 0.]\n",
      "[0.49411765 0.42352941 0.30196078 0.83346931 0.79470338 0.04923177] [0. 0. 0. 1.]\n",
      "[0.09803922 0.15294118 0.15294118 0.33424266 0.75600518 0.19199382] [1. 0. 0. 0.]\n",
      "[0.25882353 0.29411765 0.18823529 0.80940604 0.85287267 0.01072754] [0. 0. 0. 1.]\n",
      "[0.75294118 0.43529412 0.25098039 0.81302402 0.80330955 0.03958496] [0. 0. 1. 0.]\n",
      "[0.0745098  0.38039216 0.12941176 0.25289903 0.77904837 0.15778646] [0. 1. 0. 0.]\n",
      "[0.77647059 0.43137255 0.30980392 0.85407307 0.82105263 0.0325    ] [0. 0. 1. 0.]\n",
      "[0.4745098  0.49019608 0.34901961 0.36042869 0.76546266 0.19329427] [0. 0. 0. 1.]\n",
      "[0.08235294 0.14509804 0.14117647 0.27553706 0.85759268 0.12545085] [0. 0. 0. 1.]\n",
      "[0.32941176 0.27843137 0.25882353 0.28086964 0.79847226 0.17523763] [1. 0. 0. 0.]\n",
      "[0.10980392 0.44705882 0.20784314 0.24620496 0.71310567 0.22509603] [0. 1. 0. 0.]\n",
      "[0.26666667 0.29411765 0.2745098  0.3031623  0.77825388 0.19688151] [1. 0. 0. 0.]\n",
      "[0.25490196 0.28235294 0.26666667 0.31435999 0.81633494 0.18057292] [1. 0. 0. 0.]\n",
      "[0.44313725 0.52941176 0.36862745 0.59386367 0.69596325 0.09641764] [0. 0. 0. 1.]\n",
      "[0.09019608 0.4        0.14509804 0.2648365  0.7536077  0.16523437] [0. 1. 0. 0.]\n",
      "[0.08627451 0.39215686 0.16078431 0.27057359 0.70595036 0.17293945] [0. 1. 0. 0.]\n",
      "[0.05882353 0.30196078 0.1254902  0.26662424 0.76648288 0.2002474 ] [0. 1. 0. 0.]\n",
      "[0.2745098  0.25098039 0.19215686 0.27888079 0.69410043 0.18837077] [1. 0. 0. 0.]\n",
      "[0.11372549 0.4        0.16862745 0.26784086 0.74937823 0.16575846] [0. 1. 0. 0.]\n",
      "[0.25882353 0.21568627 0.24313725 0.5253041  0.61794489 0.22576009] [0. 0. 0. 1.]\n",
      "[0.25490196 0.28627451 0.2745098  0.31316994 0.82009458 0.18911296] [1. 0. 0. 0.]\n",
      "[0.11764706 0.1254902  0.10980392 0.3155041  0.79234074 0.18430176] [1. 0. 0. 0.]\n",
      "[0.31764706 0.26666667 0.27058824 0.27873334 0.78977314 0.17531576] [1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x_train)):\n",
    "    print(x_train[i], y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.3686 - loss: 1.4191\n",
      "Epoch 2/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3735 - loss: 1.4120 \n",
      "Epoch 3/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4263 - loss: 1.3974 \n",
      "Epoch 4/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4041 - loss: 1.3910  \n",
      "Epoch 5/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4179 - loss: 1.3843 \n",
      "Epoch 6/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4817 - loss: 1.3619  \n",
      "Epoch 7/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4216 - loss: 1.3671  \n",
      "Epoch 8/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4656 - loss: 1.3519 \n",
      "Epoch 9/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4279 - loss: 1.3428 \n",
      "Epoch 10/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4691 - loss: 1.3261  \n",
      "Epoch 11/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4413 - loss: 1.3163 \n",
      "Epoch 12/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4518 - loss: 1.3056  \n",
      "Epoch 13/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5413 - loss: 1.2926 \n",
      "Epoch 14/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5773 - loss: 1.2775  \n",
      "Epoch 15/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5982 - loss: 1.2639 \n",
      "Epoch 16/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6217 - loss: 1.2449 \n",
      "Epoch 17/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6375 - loss: 1.2294 \n",
      "Epoch 18/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6452 - loss: 1.2166  \n",
      "Epoch 19/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6232 - loss: 1.2024  \n",
      "Epoch 20/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7217 - loss: 1.1722 \n",
      "Epoch 21/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7259 - loss: 1.1631 \n",
      "Epoch 22/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7836 - loss: 1.1482 \n",
      "Epoch 23/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7373 - loss: 1.1291  \n",
      "Epoch 24/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7617 - loss: 1.1078 \n",
      "Epoch 25/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7451 - loss: 1.0929 \n",
      "Epoch 26/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7748 - loss: 1.0640 \n",
      "Epoch 27/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7583 - loss: 1.0530 \n",
      "Epoch 28/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7326 - loss: 1.0388 \n",
      "Epoch 29/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7614 - loss: 1.0225  \n",
      "Epoch 30/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7262 - loss: 1.0058  \n",
      "Epoch 31/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7341 - loss: 0.9982  \n",
      "Epoch 32/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7474 - loss: 0.9638  \n",
      "Epoch 33/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7324 - loss: 0.9606 \n",
      "Epoch 34/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7389 - loss: 0.9540  \n",
      "Epoch 35/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7251 - loss: 0.9472\n",
      "Epoch 36/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7474 - loss: 0.9224  \n",
      "Epoch 37/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7676 - loss: 0.8999  \n",
      "Epoch 38/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7632 - loss: 0.8885 \n",
      "Epoch 39/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7631 - loss: 0.8776 \n",
      "Epoch 40/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7572 - loss: 0.8671  \n",
      "Epoch 41/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8016 - loss: 0.8311  \n",
      "Epoch 42/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7665 - loss: 0.8323 \n",
      "Epoch 43/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8018 - loss: 0.8173  \n",
      "Epoch 44/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7677 - loss: 0.8116 \n",
      "Epoch 45/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8060 - loss: 0.7857 \n",
      "Epoch 46/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7719 - loss: 0.8014 \n",
      "Epoch 47/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7986 - loss: 0.7750  \n",
      "Epoch 48/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8133 - loss: 0.7618  \n",
      "Epoch 49/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8113 - loss: 0.7423  \n",
      "Epoch 50/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8090 - loss: 0.7728\n",
      "Epoch 51/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8406 - loss: 0.7493  \n",
      "Epoch 52/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8063 - loss: 0.7409 \n",
      "Epoch 53/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8433 - loss: 0.7289 \n",
      "Epoch 54/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8541 - loss: 0.7179 \n",
      "Epoch 55/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8742 - loss: 0.6971 \n",
      "Epoch 56/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8394 - loss: 0.7104  \n",
      "Epoch 57/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8767 - loss: 0.7001\n",
      "Epoch 58/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8405 - loss: 0.6914  \n",
      "Epoch 59/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8692 - loss: 0.6617 \n",
      "Epoch 60/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8708 - loss: 0.6458 \n",
      "Epoch 61/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8771 - loss: 0.6556 \n",
      "Epoch 62/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8865 - loss: 0.6231  \n",
      "Epoch 63/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8750 - loss: 0.6397  \n",
      "Epoch 64/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8650 - loss: 0.6339 \n",
      "Epoch 65/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8809 - loss: 0.6205 \n",
      "Epoch 66/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8604 - loss: 0.6186  \n",
      "Epoch 67/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8840 - loss: 0.5901  \n",
      "Epoch 68/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8612 - loss: 0.5924 \n",
      "Epoch 69/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8460 - loss: 0.5976  \n",
      "Epoch 70/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8754 - loss: 0.5810 \n",
      "Epoch 71/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8448 - loss: 0.5994 \n",
      "Epoch 72/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8482 - loss: 0.5808 \n",
      "Epoch 73/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8554 - loss: 0.5783  \n",
      "Epoch 74/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8783 - loss: 0.5524 \n",
      "Epoch 75/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8948 - loss: 0.5279 \n",
      "Epoch 76/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8522 - loss: 0.5465 \n",
      "Epoch 77/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8886 - loss: 0.5202  \n",
      "Epoch 78/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8856 - loss: 0.5349 \n",
      "Epoch 79/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8931 - loss: 0.5162 \n",
      "Epoch 80/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8723 - loss: 0.5228 \n",
      "Epoch 81/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8884 - loss: 0.5028 \n",
      "Epoch 82/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8985 - loss: 0.4885  \n",
      "Epoch 83/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8790 - loss: 0.4849 \n",
      "Epoch 84/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8796 - loss: 0.4818  \n",
      "Epoch 85/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9081 - loss: 0.4580 \n",
      "Epoch 86/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8855 - loss: 0.4792 \n",
      "Epoch 87/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8825 - loss: 0.4672  \n",
      "Epoch 88/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8944 - loss: 0.4734 \n",
      "Epoch 89/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8784 - loss: 0.4662 \n",
      "Epoch 90/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9007 - loss: 0.4327 \n",
      "Epoch 91/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8809 - loss: 0.4513 \n",
      "Epoch 92/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8802 - loss: 0.4621  \n",
      "Epoch 93/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9066 - loss: 0.4162 \n",
      "Epoch 94/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8929 - loss: 0.4242 \n",
      "Epoch 95/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8977 - loss: 0.4389  \n",
      "Epoch 96/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9159 - loss: 0.4153 \n",
      "Epoch 97/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8983 - loss: 0.4221 \n",
      "Epoch 98/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9068 - loss: 0.4059 \n",
      "Epoch 99/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9020 - loss: 0.4135 \n",
      "Epoch 100/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9149 - loss: 0.4011 \n",
      "Epoch 101/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9143 - loss: 0.3855 \n",
      "Epoch 102/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9117 - loss: 0.3785 \n",
      "Epoch 103/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9150 - loss: 0.3806  \n",
      "Epoch 104/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8948 - loss: 0.3927  \n",
      "Epoch 105/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8952 - loss: 0.3886 \n",
      "Epoch 106/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9142 - loss: 0.3596 \n",
      "Epoch 107/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9219 - loss: 0.3672 \n",
      "Epoch 108/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9092 - loss: 0.3722 \n",
      "Epoch 109/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9311 - loss: 0.3465 \n",
      "Epoch 110/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9089 - loss: 0.3641 \n",
      "Epoch 111/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8900 - loss: 0.3872  \n",
      "Epoch 112/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8983 - loss: 0.3768 \n",
      "Epoch 113/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8945 - loss: 0.3770\n",
      "Epoch 114/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9134 - loss: 0.3498\n",
      "Epoch 115/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8881 - loss: 0.3628  \n",
      "Epoch 116/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9110 - loss: 0.3367 \n",
      "Epoch 117/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9072 - loss: 0.3507 \n",
      "Epoch 118/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9092 - loss: 0.3427  \n",
      "Epoch 119/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9094 - loss: 0.3495 \n",
      "Epoch 120/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9307 - loss: 0.3188 \n",
      "Epoch 121/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9246 - loss: 0.3122  \n",
      "Epoch 122/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9302 - loss: 0.3070 \n",
      "Epoch 123/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9164 - loss: 0.3143 \n",
      "Epoch 124/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9295 - loss: 0.2998 \n",
      "Epoch 125/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9128 - loss: 0.3329 \n",
      "Epoch 126/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9316 - loss: 0.3257 \n",
      "Epoch 127/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9164 - loss: 0.3435 \n",
      "Epoch 128/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9190 - loss: 0.3105 \n",
      "Epoch 129/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9337 - loss: 0.2853 \n",
      "Epoch 130/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9300 - loss: 0.2983 \n",
      "Epoch 131/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9212 - loss: 0.3014 \n",
      "Epoch 132/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9352 - loss: 0.3092 \n",
      "Epoch 133/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9213 - loss: 0.3046  \n",
      "Epoch 134/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9234 - loss: 0.2971 \n",
      "Epoch 135/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9196 - loss: 0.3128 \n",
      "Epoch 136/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9386 - loss: 0.2765 \n",
      "Epoch 137/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9411 - loss: 0.2640 \n",
      "Epoch 138/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9375 - loss: 0.2895 \n",
      "Epoch 139/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9305 - loss: 0.2783 \n",
      "Epoch 140/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9374 - loss: 0.2761  \n",
      "Epoch 141/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9207 - loss: 0.2899  \n",
      "Epoch 142/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9437 - loss: 0.2540 \n",
      "Epoch 143/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9095 - loss: 0.3160 \n",
      "Epoch 144/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9194 - loss: 0.2858 \n",
      "Epoch 145/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9162 - loss: 0.2927 \n",
      "Epoch 146/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9327 - loss: 0.2703 \n",
      "Epoch 147/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9427 - loss: 0.2457 \n",
      "Epoch 148/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9187 - loss: 0.2852 \n",
      "Epoch 149/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9195 - loss: 0.2827 \n",
      "Epoch 150/150\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9329 - loss: 0.2599  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9418 - loss: 0.2673  \n",
      "Test accuracy: 0.9306 and loss: 0.3011\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=150, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_accuracy:.4f} and loss: {test_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_with_noglass.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
